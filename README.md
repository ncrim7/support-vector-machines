# DESTEK VEKTÃ–R MAKÄ°NELERÄ° (SVM)

Bu belge, **Destek VektÃ¶r Makineleri (SVM)** algoritmasÄ±nÄ±n detaylÄ± bir incelemesini iÃ§ermektedir. Bu Ã§alÄ±ÅŸma, algoritmanÄ±n teorik temelleri, matematiksel aÃ§Ä±klamalarÄ±, farklÄ± Ã§ekirdek fonksiyonlarÄ±nÄ±n kullanÄ±mÄ± ve uygulama alanlarÄ± hakkÄ±nda kapsamlÄ± bilgiler sunmaktadÄ±r.

## Ä°Ã§indekiler

1. [DESTEK VEKTÃ–R MAKÄ°NELERÄ° (SVM)](#1-destek-vektÃ¶r-makineleri-svm)
   - [GiriÅŸ](#11-giriÅŸ)
     - [AlgoritmanÄ±n TarihÃ§esi ve KeÅŸfi](#111-algoritmanÄ±n-tarihÃ§esi-ve-keÅŸfi)
     - [Nerelerde ve Neden KullanÄ±lÄ±r](#112-nerelerde-ve-neden-kullanÄ±lÄ±r)
     - [GerÃ§ek DÃ¼nya Ã–rnekleri](#113-gerÃ§ek-dÃ¼nya-Ã¶rnekleri)
   - [Teorik Temel](#12-teorik-temel)
     - [AlgoritmanÄ±n Matematiksel ve Teorik AÃ§Ä±klamasÄ±](#121-algoritmanÄ±n-matematiksel-ve-teorik-aÃ§Ä±klamasÄ±)
     - [DoÄŸrusal SVM](#122-doÄŸrusal-svm)
     - [DoÄŸrusal Olmayan SVM](#123-doÄŸrusal-olmayan-svm)
     - [Optimizasyon Problemi](#124-optimizasyon-problemi)
     - [Ã‡ekirdek Hilesi (Kernel Trick)](#125-Ã§ekirdek-hilesi-kernel-trick)
     - [Polinom Ã‡ekirdek](#126-polinom-Ã§ekirdek)
     - [RBF Ã‡ekirdek](#127-rbf-Ã§ekirdek)
     - [AlgoritmanÄ±n AvantajlarÄ± ve SÄ±nÄ±rlamalarÄ±](#128-algoritmanÄ±n-avantajlarÄ±-ve-sÄ±nÄ±rlamalarÄ±)
   - [AlgoritmanÄ±n Uygulama AlanlarÄ±](#13-algoritmanÄ±n-uygulama-alanlarÄ±)
     - [AlgoritmanÄ±n AÃ§Ä±klanmasÄ±](#131-algoritmanÄ±n-aÃ§Ä±klanmasÄ±)
     - [Algoritma](#132-algoritma)
     - [KullanÄ±m AlanlarÄ±](#133-kullanÄ±m-alanlarÄ±)
   - [Performans Analizi](#14-performans-analizi)
     - [Uzay ve Zaman KarmaÅŸÄ±klÄ±ÄŸÄ±](#141-uzay-ve-zaman-karmaÅŸÄ±klÄ±ÄŸÄ±)
     - [Optimizasyon SeÃ§enekleri](#142-optimizasyon-seÃ§enekleri)
   - [Ã‡alÄ±ÅŸma SorularÄ± ve Egzersizler](#15-Ã§alÄ±ÅŸma-sorularÄ±-ve-egzersizler)
   - [Algoritma Ã–zeti](#16-algoritma-Ã¶zeti)

## 1. DESTEK VEKTÃ–R MAKÄ°NELERÄ° (SVM)

### 1.1 GiriÅŸ

Destek VektÃ¶r Makineleri (SVM), gÃ¶zetimli Ã¶ÄŸrenme yÃ¶ntemlerinden biridir ve sÄ±nÄ±flandÄ±rma ve regresyon problemlerinde kullanÄ±lÄ±r. Bu algoritma, Ã¶zellikle yÃ¼ksek doÄŸruluk ve sÄ±nÄ±flandÄ±rma gÃ¼venilirliÄŸi saÄŸlamak amacÄ±yla tercih edilmektedir.

#### 1.1.1 AlgoritmanÄ±n TarihÃ§esi ve KeÅŸfi
Destek VektÃ¶r Makineleri (SVM), 1960'larda Vladimir Vapnik ve Alexey Chervonenkis tarafÄ±ndan 
geliÅŸtirilmeye baÅŸlanmÄ±ÅŸ bir makine Ã¶ÄŸrenmesi algoritmasÄ±dÄ±r. Fakat SVM'lerin gerÃ§ek potansiyeli, 
1990'larda Bernhard Boser, Isabelle Guyon ve Vladimir Vapnik tarafÄ±ndan tanÄ±tÄ±lan Ã§ekirdek hilesiyle 
(kernel trick) ortaya Ã§Ä±kmÄ±ÅŸtÄ±r. Bu yenilik, SVM'lerin doÄŸrusal olmayan problemlere uygulanabilmesini 
saÄŸlayarak popÃ¼lerliÄŸini bÃ¼yÃ¼k Ã¶lÃ§Ã¼de artÄ±rmÄ±ÅŸtÄ±r. 

#### 1.1.2 Nerelerde ve Neden KullanÄ±lÄ±r
Destek VektÃ¶r Makineleri (SVM), sÄ±nÄ±flandÄ±rma ve regresyon problemlerinde oldukÃ§a etkili bir 
yÃ¶ntemdir. Ã–zellikle kÃ¼Ã§Ã¼k ve orta Ã¶lÃ§ekli veri setlerinde ve yÃ¼ksek boyutlu verilerde baÅŸarÄ±lÄ± sonuÃ§lar 
verir. Ä°ÅŸte Destek VektÃ¶r Makinelerinin tercih edilme nedenlerinden bazÄ±larÄ±: 
KÃ¼Ã§Ã¼k veri setlerinde Ã¼stÃ¼n performans: Destek VektÃ¶r Makineleri (SVM), az sayÄ±da veri noktasÄ±yla 
bile Ã§ok iyi derecede genelleme yeteneÄŸi gÃ¶sterir. 
YÃ¼ksek boyutlu verilerle etkin Ã§alÄ±ÅŸma: Ã‡ekirdek hilesi sayesinde Destek VektÃ¶r Makineleri (SVM), 
yÃ¼ksek boyutlu verileri dÃ¼ÅŸÃ¼k boyutlu bir uzaya dÃ¶nÃ¼ÅŸtÃ¼rerek karmaÅŸÄ±k problemleri baÅŸarÄ±lÄ± bir ÅŸekilde 
Ã§Ã¶zebilir. 
Daha az ayarlanabilir parametre: Destek VektÃ¶r Makineleri (SVM), diÄŸer algoritmalara gÃ¶re daha az 
ayarlanabilir parametreye sahiptir, bu da modelin daha hÄ±zlÄ± ve kolay bir ÅŸekilde eÄŸitilmesini saÄŸlar. 

#### 1.1.3 GerÃ§ek DÃ¼nya Ã–rnekleri
Destek VektÃ¶r Makineleri (SVM), gÃ¶rÃ¼ntÃ¼ tanÄ±ma, el yazÄ±sÄ± tanÄ±ma, yÃ¼z tanÄ±ma, nesne tanÄ±ma gibi 
birÃ§ok alanda etkili bir ÅŸekilde kullanÄ±lmaktadÄ±r. AyrÄ±ca, spam e-posta tespiti, duygu analizi ve metinleri 
kategorize etme gibi metin sÄ±nÄ±flandÄ±rma iÅŸlemlerinde de baÅŸarÄ±lÄ±dÄ±r. Biyomedikal alanda kanser teÅŸhisi 
ve gen ifadesi analizi gibi karmaÅŸÄ±k problemleri Ã§Ã¶zmede bÃ¼yÃ¼k rol oynar. Finans sektÃ¶rÃ¼nde 
dolandÄ±rÄ±cÄ±lÄ±k tespiti ve hisse senedi fiyat tahmini gibi uygulamalarda da kullanÄ±lÄ±r. DiÄŸer alanlarda ise 
ses tanÄ±ma ve protein yapÄ±sÄ± tahmini gibi sorunlarÄ± Ã§Ã¶zmede etkilidir. SVM'nin bu kadar popÃ¼ler 
olmasÄ±nÄ±n nedeni, farklÄ± alanlardaki karmaÅŸÄ±k problemleri Ã§Ã¶zme yeteneÄŸidir. Ã–rneÄŸin, bir gÃ¶rÃ¼ntÃ¼ 
tanÄ±ma sisteminde SVM, bir gÃ¶rÃ¼ntÃ¼yÃ¼ oluÅŸturan pikselleri yÃ¼ksek boyutlu bir vektÃ¶r olarak temsil eder 
ve bu vektÃ¶rleri farklÄ± sÄ±nÄ±flara (Ã¶rneÄŸin, kedi, kÃ¶pek, araba) ait Ã¶rneklerle karÅŸÄ±laÅŸtÄ±rÄ±r. BÃ¶ylece, SVM, 
verilen bir gÃ¶rÃ¼ntÃ¼nÃ¼n hangi sÄ±nÄ±fa ait olduÄŸunu doÄŸru bir ÅŸekilde tahmin edebilir. 
<img src="https://github.com/ncrim7/support-vector-machines/blob/main/img/1_fpDngO6lM5pDeIPOOezK1g_op.webp" width="auto" height="auto">
<img src="https://github.com/ncrim7/support-vector-machines/blob/main/img/7364c7c7885b8652083ac6ff7de229ff.jpg" width="auto" height="auto">

### 1.2 Teorik Temel

SVM'nin matematiksel temeli, veri noktalarÄ±nÄ± doÄŸrusal ve doÄŸrusal olmayan sÄ±nÄ±flara ayÄ±rmaya yÃ¶nelik optimizasyon tekniklerine dayanÄ±r.

#### 1.2.1 AlgoritmanÄ±n Matematiksel ve Teorik AÃ§Ä±klamasÄ±
Veri sÄ±nÄ±flandÄ±rma, makine Ã¶ÄŸreniminde yaygÄ±n bir gÃ¶revdir. Belirli veri noktalarÄ±nÄ±n her birinin iki 
sÄ±nÄ±ftan birine ait olduÄŸunu ve amacÄ±n yeni bir veri noktasÄ±nÄ±n hangi sÄ±nÄ±fta olacaÄŸÄ±nÄ± belirlemek 
olduÄŸunu varsayalÄ±m. Destek VektÃ¶r Makineleri (SVM) durumunda, bir veri noktasÄ± p-boyutlu bir 
vektÃ¶r (p sayÄ±sÄ± kadar listeden oluÅŸan) olarak gÃ¶rÃ¼lÃ¼r ve bu tÃ¼r noktalarÄ± (p-1)-boyutlu bir hiper dÃ¼zlemle 
ayÄ±rÄ±p ayÄ±ramayacaÄŸÄ±mÄ±zÄ± bilmek isteriz. Buna doÄŸrusal sÄ±nÄ±flandÄ±rÄ±cÄ± denir. Veriyi sÄ±nÄ±flandÄ±rabilecek 
birÃ§ok hiper dÃ¼zlem vardÄ±r. En iyi hiper dÃ¼zlem olarak makul bir seÃ§enek, iki sÄ±nÄ±f arasÄ±ndaki en bÃ¼yÃ¼k 
ayrÄ±mÄ± veya marjÄ± temsil eden hiper dÃ¼zlemdir. Yani, her iki yandaki en yakÄ±n veri noktasÄ±na olan mesafe 
maksimize edilecek ÅŸekilde hiper dÃ¼zlemi seÃ§iyoruz. BÃ¶yle bir hiper dÃ¼zlem varsa, buna maksimum 
marjlÄ± hiper dÃ¼zlem denir ve tanÄ±mladÄ±ÄŸÄ± doÄŸrusal sÄ±nÄ±flandÄ±rÄ±cÄ±ya maksimum marjlÄ± sÄ±nÄ±flandÄ±rÄ±cÄ± denir; 
ya da eÅŸdeÄŸer olarak, optimal stabiliteye sahip algÄ±layÄ±cÄ± denir. 
Daha resmi olarak, bir Destek VektÃ¶r Makinesi (SVM), yÃ¼ksek veya sonsuz boyutlu bir uzayda hiper 
dÃ¼zlem veya hiper dÃ¼zlemler kÃ¼mesi oluÅŸturur. Bu, sÄ±nÄ±flandÄ±rma, regresyon veya aykÄ±rÄ± deÄŸerlerin 
tespiti gibi diÄŸer gÃ¶revler iÃ§in kullanÄ±labilir. Sezgisel olarak, iyi bir ayrÄ±m, herhangi bir sÄ±nÄ±fÄ±n en yakÄ±n 
eÄŸitim verisi noktasÄ±na en bÃ¼yÃ¼k mesafeyi (iÅŸlevsel marj olarak adlandÄ±rÄ±lÄ±r) sahip olan hiper dÃ¼zlem 
tarafÄ±ndan saÄŸlanÄ±r. Genelde marj ne kadar bÃ¼yÃ¼k olursa, sÄ±nÄ±flandÄ±rÄ±cÄ±nÄ±n genelleme hatasÄ± o kadar 
dÃ¼ÅŸÃ¼k olur. Daha dÃ¼ÅŸÃ¼k bir genelleme hatasÄ±, uygulayÄ±cÄ±nÄ±n aÅŸÄ±rÄ± Ã¶ÄŸrenim (overfitting) yaÅŸama 
olasÄ±lÄ±ÄŸÄ±nÄ±n daha dÃ¼ÅŸÃ¼k olduÄŸu anlamÄ±na gelir. 
Ancak, baÅŸlangÄ±Ã§taki problem sonlu boyutlu bir uzayda ifade edilse de, genellikle ayrÄ±lmasÄ± gereken 
kÃ¼meler bu uzayda doÄŸrusal olarak ayrÄ±labilir deÄŸildir. Bu nedenle, orijinal sonlu boyutlu uzayÄ±n Ã§ok 
daha yÃ¼ksek boyutlu bir uzaya dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmesi Ã¶nerildi; bu ÅŸekilde ayrÄ±m daha kolay yapÄ±labilir. 
Hesaplama yÃ¼kÃ¼nÃ¼ makul dÃ¼zeyde tutmak iÃ§in, SVM ÅŸemalarÄ±nda kullanÄ±lan dÃ¶nÃ¼ÅŸÃ¼mler, giriÅŸ veri 
vektÃ¶r Ã§iftlerinin nokta Ã§arpÄ±mlarÄ±nÄ±n, orijinal uzaydaki deÄŸiÅŸkenler cinsinden kolayca 
hesaplanabilmesini saÄŸlamak Ã¼zere tasarlanmÄ±ÅŸtÄ±r. Bu, problem iÃ§in uygun bir Ã§ekirdek fonksiyonu 
k(x,y) tanÄ±mlanarak yapÄ±lÄ±r. Daha yÃ¼ksek boyutlu uzaydaki hiper dÃ¼zlemler, bu uzaydaki bir vektÃ¶rle 
nokta Ã§arpÄ±mÄ± sabit olan noktalar kÃ¼mesi olarak tanÄ±mlanÄ±r. Hiper dÃ¼zlemleri tanÄ±mlayan vektÃ¶rler, veri 
tabanÄ±nda bulunan Ã¶zellik vektÃ¶rlerinin gÃ¶rÃ¼ntÃ¼lerinin Î±i parametreleriyle lineer kombinasyonlarÄ± 
olarak seÃ§ilebilir. Bu ÅŸekilde bir hiper dÃ¼zlem seÃ§ildiÄŸinde, Ã¶zellik uzayÄ±ndaki x noktalarÄ±, Î£i.Î±i.k(xi,x) 
= sabit baÄŸÄ±ntÄ±sÄ±yla tanÄ±mlanÄ±r. k(x,y) x'den uzaklaÅŸtÄ±kÃ§a kÃ¼Ã§Ã¼lÃ¼rse, toplamÄ±n her bir terimi, test 
noktasÄ±nÄ±n x'e karÅŸÄ±lÄ±k gelen veri tabanÄ± noktasÄ±na yakÄ±nlÄ±ÄŸÄ±nÄ± Ã¶lÃ§er. Bu ÅŸekilde, yukarÄ±daki Ã§ekirdeklerin 
toplamÄ±, her bir test noktasÄ±nÄ±n ayrÄ±m yapÄ±lacak iki kÃ¼meden hangisine daha yakÄ±n olduÄŸunu Ã¶lÃ§mek iÃ§in 
kullanÄ±labilir. Bu, herhangi bir hiper dÃ¼zleme dÃ¶nÃ¼ÅŸtÃ¼rÃ¼len x noktalarÄ±nÄ±n setinin oldukÃ§a karmaÅŸÄ±k 
olabileceÄŸi ve bu nedenle, orijinal uzayda konveks olmayan kÃ¼meler arasÄ±nda Ã§ok daha karmaÅŸÄ±k 
ayrÄ±mlar yapÄ±lmasÄ±na olanak tanÄ±dÄ±ÄŸÄ± anlamÄ±na gelir.

#### 1.2.2 DoÄŸrusal SVM
Bize (ğ’™ğŸ,ğ’šğŸ),â€¦,(ğ’™ğ’,ğ’šğ’) formundaki n noktadan oluÅŸan bir eÄŸitim veri kÃ¼mesi verilir; burada y, ya 1 
ya da -1â€™dir, her biri x noktasÄ±nÄ±n ait olduÄŸu sÄ±nÄ±fÄ± belirtir. Her x, p boyutlu bir gerÃ§ek vektÃ¶rdÃ¼r. ğ’šğ’Š = 1 
olan x nokta grubunu, ğ’šğ’Š = 1 olan nokta grubundan ayÄ±ran "maksimum kenar hiperdÃ¼zlemini" bulmak 
istiyoruz; hiperdÃ¼zlem ve her iki gruptan en yakÄ±n x noktasÄ± maksimize edilir. Herhangi bir hiperdÃ¼zlem, 
ï¿½
ï¿½ğ‘»ğ’™âˆ’ğ’ƒ=ğŸ 'Ä± saÄŸlayan x noktalarÄ± kÃ¼mesi olarak yazÄ±labilir; burada w, hiperdÃ¼zlemin (zorunlu olarak 
normalleÅŸtirilmiÅŸ olmasÄ± gerekmez) normal vektÃ¶rÃ¼dÃ¼r. Bu, wâ€™nin mutlaka bir birim vektÃ¶r olmamasÄ± 
dÄ±ÅŸÄ±nda Hesse normal formuna Ã§ok benzer. Parametre, hiperdÃ¼zlemin w normal vektÃ¶rÃ¼ boyunca 
orijinden uzaklÄ±ÄŸÄ±nÄ± belirler. UyarÄ±: Konuyla ilgili literatÃ¼rÃ¼n Ã§oÄŸu, iki sÄ±nÄ±ftan Ã¶rneklerle eÄŸitilmiÅŸ bir 
SVM iÃ§in Maksimum kenar boÅŸluÄŸu hiperdÃ¼zlemi ve kenar boÅŸluklarÄ± olacak ÅŸekilde Ã¶nyargÄ±yÄ± tanÄ±mlar. 
Kenardaki Ã¶rneklere destek vektÃ¶rleri denir. ğ’˜ğ‘»ğ’™ + ğ’ƒ = ğŸ 
<img src="https://github.com/ncrim7/support-vector-machines/blob/main/img/SVM_margin.png" width="auto" height="auto">

#### 1.2.3 DoÄŸrusal Olmayan SVM
Destek VektÃ¶r Makineleri (SVM), verileri optimal bir hiper dÃ¼zlem ile ayÄ±rmayÄ± amaÃ§layan gÃ¼Ã§lÃ¼ 
makine Ã¶ÄŸrenmesi algoritmalarÄ±dÄ±r, ancak tÃ¼m veri kÃ¼meleri doÄŸrusal olarak ayrÄ±labilir olmayabilir; bu 
durumda doÄŸrusal olmayan SVM sÄ±nÄ±flandÄ±rÄ±cÄ±lar devreye girer. DoÄŸrusal olmayan veri kÃ¼meleri, bir 
Ã§izgi veya dÃ¼zlem ile ayrÄ±lamayan, Ã¶rneÄŸin bir dairenin iÃ§indeki ve dÄ±ÅŸÄ±ndaki noktalarÄ± ayÄ±ran verilerdir 
ve bu tÃ¼r veri kÃ¼meleri iÃ§in klasik SVM yÃ¶ntemleri yetersiz kalÄ±r. DoÄŸrusal olmayan SVM sÄ±nÄ±flandÄ±rÄ±cÄ±, 
Ã§ekirdek hilesi kullanarak bu tÃ¼r verileri etkili bir ÅŸekilde sÄ±nÄ±flandÄ±rabilir ve farklÄ± Ã§ekirdek 
fonksiyonlarÄ± ve hiperparametre ayarlarÄ± ile Ã§eÅŸitli problemlere uyum saÄŸlar. Bununla birlikte, bÃ¼yÃ¼k 
veri setlerinde performans dÃ¼ÅŸebilir ve hiperparametre seÃ§imi dikkatle yapÄ±lmalÄ±dÄ±r; bu algoritmanÄ±n 
gÃ¼Ã§lÃ¼ yÃ¶nleri arasÄ±nda kÃ¼Ã§Ã¼k veri setlerinde iyi performans gÃ¶stermesi, yÃ¼ksek boyutlu verilerle 
Ã§alÄ±ÅŸabilmesi ve az sayÄ±da ayarlanabilir parametre gerektirmesi yer alÄ±rken, sÄ±nÄ±rlamalarÄ± arasÄ±nda bÃ¼yÃ¼k 
veri setlerinde yavaÅŸ Ã§alÄ±ÅŸmasÄ±, karmaÅŸÄ±k doÄŸrusal olmayan problemlerde performans dÃ¼ÅŸÃ¼ÅŸÃ¼ ve aykÄ±rÄ± 
deÄŸerlere duyarlÄ±lÄ±k bulunmaktadÄ±r. SonuÃ§ olarak, SVM, Ã§eÅŸitli alanlarda baÅŸarÄ±yla kullanÄ±lan gÃ¼Ã§lÃ¼ bir 
sÄ±nÄ±flandÄ±rma algoritmasÄ±dÄ±r; ancak, algoritma seÃ§imi yaparken verinin Ã¶zellikleri, problemin 
karmaÅŸÄ±klÄ±ÄŸÄ± ve istenen performans gibi faktÃ¶rler dikkate alÄ±nmalÄ±dÄ±r.
<img src="https://github.com/ncrim7/support-vector-machines/blob/main/img/Ekran%20Al%C4%B1nt%C4%B1s%C4%B1.PNG" width="auto" height="auto">

#### 1.2.4 Optimizasyon Problemi
SVM algoritmasÄ±nÄ±n baÅŸarÄ±sÄ±nÄ±n merkezinde, veri noktalarÄ±nÄ± en iyi ÅŸekilde ayÄ±racak bir hiper dÃ¼zlemi 
bulmayÄ± hedefleyen bir optimizasyon problemi vardÄ±r. Bu problemi Ã§Ã¶zmek, SVM'nin etkinliÄŸini 
belirleyen en kritik faktÃ¶rlerden biridir. SVM'deki asÄ±l amaÃ§, verilen bir veri kÃ¼mesini en bÃ¼yÃ¼k marjla 
ayÄ±ran bir hiper dÃ¼zlem bulmaktÄ±r. Bu, ÅŸu ÅŸekilde matematiksel bir optimizasyon problemine 
dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r: ğ‘´ğ’Šğ’ğ’Šğ’ğ’Šğ’›ğ’†: ğŸ ğŸ
 â„ âˆ£âˆ£ ğ’˜âˆ£âˆ£ğŸ ğ‘ºğ’–ğ’ƒğ’‹ğ’†ğ’„ğ’• ğ’•ğ’: ğ’šğ’Š(ğ’˜ğ‘» â‹…ğ’™ğ’Š +ğ’ƒ) â‰¥ ğŸ, ğ’‡ğ’ğ’“ ğ’Š = ğŸ,...,ğ’ 

Bu formÃ¼lde: 
w: Hiper dÃ¼zlemin normal vektÃ¶rÃ¼ 
b: Hiper dÃ¼zlemin kaydÄ±rma miktarÄ± 
ï¿½
ï¿½ğ’Š: i'inci veri noktasÄ± 
ï¿½
ï¿½ğ’Š: i'inci veri noktasÄ±nÄ±n etiketi (1 veya -1) 
Bu problem, kÄ±sÄ±tlÄ± kuadratik programlama (quadratic programming) olarak bilinir. Ã‡Ã¶zÃ¼mÃ¼, Ã¶zel 
algoritmalar ve yazÄ±lÄ±mlar gerektirir. 

#### 1.2.5 Ã‡ekirdek Hilesi (Kernel Trick)
Ã‡ekirdek Hilesi (Kernel Trick), Destek VektÃ¶r Makineleri (SVM) algoritmasÄ±nÄ±n en gÃ¼Ã§lÃ¼ 
Ã¶zelliklerinden biridir ve bu teknik, doÄŸrusal olmayan ayrÄ±labilir veri kÃ¼melerini yÃ¼ksek boyutlu bir 
uzaya dÃ¶nÃ¼ÅŸtÃ¼rerek, orada doÄŸrusal bir ayÄ±rma yapÄ±lmasÄ±nÄ± saÄŸlar, bu sayede daha karmaÅŸÄ±k problemler 
Ã§Ã¶zÃ¼lebilir. GerÃ§ek dÃ¼nyadaki birÃ§ok veri kÃ¼mesi doÄŸrusal bir Ã§izgi veya hiper dÃ¼zlem ile tam olarak 
ayrÄ±lamaz ve verileri yÃ¼ksek boyutlu bir uzaya dÃ¶nÃ¼ÅŸtÃ¼rerek daha karmaÅŸÄ±k iliÅŸkiler yakalanabilir, ancak 
yÃ¼ksek boyutlu uzaylarda hesaplamalarÄ±n karmaÅŸÄ±klÄ±ÄŸÄ± artar. Bu nedenle, veri noktalarÄ±nÄ± yÃ¼ksek 
boyutlu bir uzaya dÃ¶nÃ¼ÅŸtÃ¼rmek yerine, bu dÃ¶nÃ¼ÅŸÃ¼mÃ¼n sonucunda elde edilecek iÃ§ Ã§arpÄ±mlarÄ± doÄŸrudan 
hesaplayan bir fonksiyon (Ã§ekirdek fonksiyonu) kullanÄ±lÄ±r. FarklÄ± Ã§ekirdek fonksiyonlarÄ±, farklÄ± tÃ¼rdeki 
doÄŸrusal olmayan iliÅŸkileri yakalar; Ã¶rneÄŸin, lineer Ã§ekirdek doÄŸrusal ayrÄ±m iÃ§in kullanÄ±lÄ±rken, polinom 
Ã§ekirdek daha karmaÅŸÄ±k, polinomsal iliÅŸkiler iÃ§in ve RBF (Radial Basis Function) Ã§ekirdek en yaygÄ±n 
kullanÄ±lan Ã§ekirdek olarak veri noktalarÄ± arasÄ±ndaki benzerliÄŸi Ã¶lÃ§er. Ã‡ekirdek hilesi, verileri aÃ§Ä±kÃ§a 
yÃ¼ksek boyutlu bir uzaya dÃ¶nÃ¼ÅŸtÃ¼rmeye gerek kalmadan, Ã§ekirdek fonksiyonlarÄ± aracÄ±lÄ±ÄŸÄ±yla bu 
dÃ¶nÃ¼ÅŸÃ¼mÃ¼ ima eder ve yÃ¼ksek boyutlu uzaylardaki hesaplamalar yerine Ã§ekirdek matrisi adÄ± verilen bir 
matris Ã¼zerinde iÅŸlemler yaparak hesaplama karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± azaltÄ±r. FarklÄ± Ã§ekirdek fonksiyonlarÄ± 
sayesinde, farklÄ± tÃ¼rdeki verilere uyum saÄŸlamak mÃ¼mkÃ¼ndÃ¼r. Ã–rneÄŸin, RBF Ã§ekirdeÄŸi iki veri noktasÄ± 
arasÄ±ndaki Ã–klid uzaklÄ±ÄŸÄ±na baÄŸlÄ± olarak bir benzerlik Ã¶lÃ§Ã¼sÃ¼ verir ve genellikle doÄŸrusal olmayan ayrÄ±m 
problemleri iÃ§in iyi sonuÃ§lar verir. Ã‡ekirdek hilesi, SVM algoritmasÄ±nÄ±n gÃ¼cÃ¼nÃ¼ artÄ±ran Ã¶nemli bir 
tekniktir, doÄŸrusal olmayan ayrÄ±labilir veri kÃ¼melerini baÅŸarÄ±yla sÄ±nÄ±flandÄ±rmamÄ±zÄ± saÄŸlar, farklÄ± Ã§ekirdek 
fonksiyonlarÄ± seÃ§erek, farklÄ± tÃ¼rdeki veriler iÃ§in en uygun modeli elde edebiliriz, doÄŸrusal olmayan veri 
kÃ¼melerini ayÄ±rabilir, verileri yÃ¼ksek boyutlu uzaylara dÃ¶nÃ¼ÅŸtÃ¼rerek daha karmaÅŸÄ±k iliÅŸkileri yakalar, 
farklÄ± veri tÃ¼rlerine uyum saÄŸlar ve Ã§ekirdek matrisi sayesinde hesaplama yÃ¼kÃ¼nÃ¼ azaltÄ±r. 

<img src="https://github.com/ncrim7/support-vector-machines/blob/main/img/Kernel_trick_idea.svg.png" width="auto" height="auto">

#### 1.2.6 Polinom Ã‡ekirdek
ve bu fonksiyon, verileri daha yÃ¼ksek boyutlu bir uzaya dÃ¶nÃ¼ÅŸtÃ¼rerek doÄŸrusal olmayan verilerin daha 
iyi sÄ±nÄ±flandÄ±rÄ±lmasÄ±nÄ± saÄŸlar. Polinom Ã§ekirdek, iki veri noktasÄ± arasÄ±ndaki iliÅŸkiyi bir polinom fonksiyonu ile ifade eder ve bÃ¶ylece veriler arasÄ±ndaki daha karmaÅŸÄ±k iliÅŸkiler yakalanabilir; genel olarak 
K(x, y) = (Î³ <x, y> + r)^d formÃ¼lÃ¼ ile tanÄ±mlanÄ±r, burada K(x, y) x ve y veri noktalarÄ± arasÄ±ndaki Ã§ekirdek 
deÄŸerini, Î³ Ã§ekirdeÄŸin geniÅŸliÄŸini, <x, y> x ve y vektÃ¶rlerinin iÃ§ Ã§arpÄ±mÄ±nÄ±, r sabit bir terimi ve d 
polinomun derecesini ifade eder. Polinomun derecesi olan d, polinomun karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± belirler; d deÄŸeri 
arttÄ±kÃ§a polinom daha karmaÅŸÄ±k hale gelir ve daha yÃ¼ksek dereceden etkileÅŸimler yakalanabilir, ancak 
Ã§ok yÃ¼ksek dereceler modelin aÅŸÄ±rÄ± Ã¶ÄŸrenme riskini artÄ±rabilir. Polinom Ã§ekirdekler, doÄŸrusal olarak 
ayrÄ±lamayan verileri daha yÃ¼ksek boyutlu bir uzaya dÃ¶nÃ¼ÅŸtÃ¼rerek orada doÄŸrusal bir hiper dÃ¼zlem ile 
ayÄ±rmayÄ± saÄŸlar, veriler arasÄ±ndaki karmaÅŸÄ±k ve doÄŸrusal olmayan iliÅŸkileri yakalayabilir ve d 
parametresi sayesinde farklÄ± derecelerdeki polinomlar kullanÄ±larak modelin karmaÅŸÄ±klÄ±ÄŸÄ± ayarlanabilir. 
Polinom Ã§ekirdek, veriler arasÄ±nda polinom bir iliÅŸki olduÄŸu dÃ¼ÅŸÃ¼nÃ¼ldÃ¼ÄŸÃ¼nde, verilerin doÄŸrusal 
olmayan bir yapÄ±ya sahip olduÄŸu durumlarda ve diÄŸer Ã§ekirdek fonksiyonlarÄ±nÄ±n (Ã¶rneÄŸin RBF) iyi sonuÃ§ 
vermediÄŸi durumlarda kullanÄ±lÄ±r. Polinom Ã§ekirdek, SVM'lerde doÄŸrusal olmayan verileri 
sÄ±nÄ±flandÄ±rmak iÃ§in gÃ¼Ã§lÃ¼ bir araÃ§tÄ±r ve veriler arasÄ±ndaki polinom iliÅŸkileri yakalayarak daha iyi bir 
model oluÅŸturulmasÄ±nÄ± saÄŸlar, ancak doÄŸru parametrelerin seÃ§ilmesi ve aÅŸÄ±rÄ± Ã¶ÄŸrenme riskine dikkat 
edilmesi Ã¶nemlidir. Bu Ã§ekirdek, verileri daha yÃ¼ksek boyutlu bir uzaya dÃ¶nÃ¼ÅŸtÃ¼rerek doÄŸrusal olmayan 
iliÅŸkileri yakalar ve d parametresi, polinomun derecesini belirler; doÄŸru parametre seÃ§imi, modelin 
baÅŸarÄ±sÄ± iÃ§in kritik Ã¶neme sahiptir. 

<img src="https://github.com/ncrim7/support-vector-machines/blob/main/img/Ekran%20Al%C4%B1nt%C4%B1s%C4%B12.PNG" width="auto" height="auto">

#### 1.2.7 RBF Ã‡ekirdek
Radyal Temel Fonksiyon (RBF) Ã§ekirdek, Destek VektÃ¶r Makinelerinde (SVM) sÄ±kÃ§a kullanÄ±lan ve 
doÄŸrusal olmayan verilerin sÄ±nÄ±flandÄ±rÄ±lmasÄ±nda etkili olan popÃ¼ler bir Ã§ekirdek tÃ¼rÃ¼dÃ¼r. RBF Ã§ekirdek, 
iki veri noktasÄ± arasÄ±ndaki Ã–klid uzaklÄ±ÄŸÄ±na dayalÄ± bir benzerlik Ã¶lÃ§Ã¼sÃ¼ saÄŸlar ve iki nokta birbirine ne 
kadar yakÄ±nsa, Ã§ekirdek deÄŸeri o kadar bÃ¼yÃ¼k olur, bÃ¶ylece verilerin yerel yapÄ±sÄ± daha iyi yakalanÄ±r. 
Matematiksel olarak ğ‘²(ğ’™,ğ’š) = ğ’†ğ’™ğ’‘(âˆ’ğœ¸ ||ğ’™ âˆ’ ğ’š||ğŸ) formÃ¼lÃ¼ ile ifade edilir, burada K(x, y) x ve y veri 
noktalarÄ± arasÄ±ndaki Ã§ekirdek deÄŸerini, Î³ Ã§ekirdeÄŸin geniÅŸliÄŸini kontrol eden bir parametreyi ve ||x-y|| x 
ve y vektÃ¶rleri arasÄ±ndaki Ã–klid uzaklÄ±ÄŸÄ±nÄ± temsil eder. Î³ parametresi Ã§ekirdeÄŸin ne kadar geniÅŸ bir alana 
etki edeceÄŸini belirler; kÃ¼Ã§Ã¼k bir Î³ deÄŸeri Ã§ekirdeÄŸin sadece yakÄ±n noktalara odaklanmasÄ±na neden 
olurken, bÃ¼yÃ¼k bir Î³ deÄŸeri daha uzak noktalarÄ±n da etkileÅŸimini saÄŸlar. RBF Ã§ekirdek, doÄŸrusal olmayan 
verileri yÃ¼ksek boyutlu bir uzaya dÃ¶nÃ¼ÅŸtÃ¼rerek doÄŸrusal bir hiper dÃ¼zlem ile ayÄ±rmayÄ± saÄŸlar, verilerin 
yerel yapÄ±sÄ±nÄ± yakalar ve birÃ§ok farklÄ± veri tÃ¼rÃ¼nde iyi sonuÃ§lar verir. RBF Ã§ekirdek genellikle iyi 
performans gÃ¶sterir ve aÅŸÄ±rÄ± Ã¶ÄŸrenme riskini azaltabilir, ancak Î³ parametresinin doÄŸru seÃ§ilmesi 
Ã¶nemlidir ve bÃ¼yÃ¼k veri setlerinde hesaplama maliyeti artabilir. Verilerin doÄŸrusal olmayan bir yapÄ±ya 
sahip olduÄŸu durumlarda, verilerin yerel yapÄ±sÄ±nÄ±n Ã¶nemli olduÄŸu durumlarda ve diÄŸer Ã§ekirdek 
fonksiyonlarÄ±nÄ±n iyi sonuÃ§ vermemesi durumunda RBF Ã§ekirdek tercih edilir. Ã–zetle, RBF Ã§ekirdek 
SVM'lerde doÄŸrusal olmayan verileri sÄ±nÄ±flandÄ±rmak iÃ§in gÃ¼Ã§lÃ¼ ve popÃ¼ler bir araÃ§tÄ±r, verilerin yerel 
yapÄ±sÄ±nÄ± yakalayarak daha iyi bir model oluÅŸturulmasÄ±nÄ± saÄŸlar ve doÄŸru parametre seÃ§imi, modelin 
baÅŸarÄ±sÄ± iÃ§in kritik Ã¶neme sahiptir.

<img src="https://github.com/ncrim7/support-vector-machines/blob/main/img/svm_kernels%20intro%20(1).png" width="auto" height="auto">

#### 1.2.8 AlgoritmanÄ±n AvantajlarÄ± ve SÄ±nÄ±rlamalarÄ±
Destek VektÃ¶r Makineleri (SVM) algoritmasÄ±, kÃ¼Ã§Ã¼k veri setlerinde iyi performans gÃ¶stermesi, yÃ¼ksek boyutlu verilerle Ã§alÄ±ÅŸma kabiliyeti, az sayÄ±da ayarlanabilir parametre gerektirmesi ve genelleme 
yeteneÄŸinin yÃ¼ksek olmasÄ± gibi avantajlara sahiptir; bu avantajlar sayesinde az sayÄ±da veri noktasÄ±yla 
bile iyi genelleme yeteneÄŸi gÃ¶sterir, Ã§ekirdek hilesi ile yÃ¼ksek boyutlu verileri dÃ¼ÅŸÃ¼k boyutlu bir uzaya 
dÃ¶nÃ¼ÅŸtÃ¼rebilir, hiper parametre sayÄ±sÄ±nÄ±n az olmasÄ± modelin eÄŸitimini kolaylaÅŸtÄ±rÄ±r ve maksimum marj 
ilkesi sayesinde aÅŸÄ±rÄ± Ã¶ÄŸrenme riskini azaltÄ±r; fakat bunun yanÄ± sÄ±ra, SVM'nin bÃ¼yÃ¼k veri setlerinde yavaÅŸ 
Ã§alÄ±ÅŸmasÄ±, hiper parametre seÃ§imi, doÄŸrusal olmayan problemlerde karmaÅŸÄ±klÄ±k ve aykÄ±rÄ± deÄŸerlere 
duyarlÄ±lÄ±k gibi sÄ±nÄ±rlamalarÄ± da mevcuttur; Ã¶zellikle veri seti bÃ¼yÃ¼dÃ¼kÃ§e eÄŸitim sÃ¼resinin artmasÄ±, 
Ã§ekirdek fonksiyonu ve diÄŸer parametrelerin doÄŸru seÃ§iminin model baÅŸarÄ±sÄ±ndaki Ã¶nemi, Ã§ok karmaÅŸÄ±k 
doÄŸrusal olmayan problemlerde performansÄ±n dÃ¼ÅŸmesi ve aykÄ±rÄ± deÄŸerlerin hiper dÃ¼zlemin konumunu 
etkileyerek modelin performansÄ±nÄ± olumsuz etkilemesi gibi; sonuÃ§ olarak, Destek VektÃ¶r Makineleri 
(SVM) gÃ¼Ã§lÃ¼ bir sÄ±nÄ±flandÄ±rma algoritmasÄ± olup, birÃ§ok farklÄ± alanda baÅŸarÄ±lÄ± bir ÅŸekilde kullanÄ±lmakta 
olup, algoritma seÃ§imi yaparken verinin Ã¶zellikleri, problemin karmaÅŸÄ±klÄ±ÄŸÄ± ve istenen performans gibi 
faktÃ¶rlerin dikkate alÄ±nmasÄ± gerektiÄŸi unutulmamalÄ±dÄ±r. 

### 1.3 AlgoritmanÄ±n Uygulama AlanlarÄ±

#### 1.3.1 AlgoritmanÄ±n AÃ§Ä±klanmasÄ±
AÅŸaÄŸÄ±daki Ã¶rnek kod, Breast Cancer (meme kanseri) veri seti Ã¼zerinde SVM algoritmasÄ±nÄ± geliÅŸmiÅŸ parametrelerle kullanarak en iyi sonucu elde etmeyi amaÃ§lar. Bu veri seti, tÄ±pta sÄ±kÃ§a kullanÄ±lan bir Ã¶rnek olduÄŸu iÃ§in SVMâ€™nin en yaygÄ±n kullanÄ±m alanlarÄ±ndan birine (saÄŸlÄ±k, hastalÄ±k teÅŸhisi) iliÅŸkin bir senaryoyu temsil eder.

#### 1.3.2 Algoritma
```python
import numpy as np
import pandas as pd

import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    roc_curve,
    auc,
    precision_recall_curve
)

# ====================== 1. VERÄ° HAZIRLAMA ======================
data = load_breast_cancer()
X, y = data.data, data.target

# EÄŸitim / test olarak ayÄ±r
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.30,
    stratify=y,
    random_state=42
)

# ====================== 2. PIPELINE OLUÅTURMA ======================
# StandardScaler + SVC
pipeline = Pipeline([
    ("scaler", StandardScaler()),
    ("svc", SVC(probability=True, random_state=42))
])

# GridSearch iÃ§in hiperparametre aralÄ±ÄŸÄ±
param_grid = {
    "svc__kernel": ["linear", "rbf", "poly"],
    "svc__C": [0.01, 0.1, 1, 10, 100],
    "svc__gamma": ["scale", 0.1, 1, 10],
    "svc__degree": [2, 3]  # 'poly' kernel iÃ§in dereceler
}

# ====================== 3. MODEL EÄÄ°TÄ°MÄ° (GridSearchCV) ======================
grid_search = GridSearchCV(
    pipeline,
    param_grid,
    cv=5,
    scoring="accuracy",
    n_jobs=-1,
    verbose=1
)
grid_search.fit(X_train, y_train)

print("En iyi parametreler:", grid_search.best_params_)
print(f"En iyi CV skoru: {grid_search.best_score_:.4f}")

best_model = grid_search.best_estimator_

# ====================== 4. TAHMÄ°N VE DEÄERLENDÄ°RME ======================
y_pred = best_model.predict(X_test)
accuracy = best_model.score(X_test, y_test)

print("\n-- Test Seti SonuÃ§larÄ± --")
print(f"DoÄŸruluk (Accuracy): {accuracy:.4f}")
print(classification_report(y_test, y_pred, target_names=["Kanser Yok", "Kanser Var"]))

# KarÄ±ÅŸÄ±klÄ±k Matrisi
cm = confusion_matrix(y_test, y_pred)
print("KarÄ±ÅŸÄ±klÄ±k Matrisi:\n", cm)

# Tahmin olasÄ±lÄ±klarÄ± (ROC ve PR eÄŸrileri iÃ§in)
y_score = best_model.predict_proba(X_test)[:, 1]

# ROC EÄŸrisi
fpr, tpr, _ = roc_curve(y_test, y_score)
roc_auc = auc(fpr, tpr)

# Precision-Recall EÄŸrisi
precision, recall, _ = precision_recall_curve(y_test, y_score)

# ====================== 5. GELÄ°ÅMÄ°Å PLOTLY GÃ–RSELLEÅTÄ°RME ======================
# 3 alt grafik (1 satÄ±r, 3 sÃ¼tun): 
#   1) KarÄ±ÅŸÄ±klÄ±k Matrisi, 2) ROC EÄŸrisi, 3) Precision-Recall EÄŸrisi
fig = make_subplots(
    rows=1, cols=3,
    subplot_titles=["KarÄ±ÅŸÄ±klÄ±k Matrisi", "ROC EÄŸrisi", "Precision-Recall EÄŸrisi"],
    horizontal_spacing=0.08
)

# -----------------------------------------------------------
# (a) KarÄ±ÅŸÄ±klÄ±k Matrisi (Heatmap)
# -----------------------------------------------------------
# Confusion Matrix'i DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼rerek annot iÃ§in metin hazÄ±rlama
cm_df = pd.DataFrame(cm, index=["GerÃ§ek: Yok", "GerÃ§ek: Var"], columns=["Tahmin: Yok", "Tahmin: Var"])
annot_text = cm_df.values.astype(str)

heatmap = go.Heatmap(
    z=cm_df.values,
    x=cm_df.columns,
    y=cm_df.index,
    colorscale="Blues",
    showscale=True,
    text=annot_text,
    texttemplate="%{text}",
    textfont={"size": 14},
    hovertemplate="GerÃ§ek: %{y}<br>Tahmin: %{x}<br>Adet: %{z}<extra></extra>"
)

fig.add_trace(heatmap, row=1, col=1)

# -----------------------------------------------------------
# (b) ROC EÄŸrisi
# -----------------------------------------------------------
roc_curve_trace = go.Scatter(
    x=fpr,
    y=tpr,
    mode="lines",
    line=dict(color="firebrick", width=3),
    name=f"ROC AUC = {roc_auc:.2f}",
    hovertemplate="FPR: %{x:.2f}<br>TPR: %{y:.2f}<extra></extra>"
)

# Rastgele sÄ±nÄ±flandÄ±rma Ã§izgisi
roc_line = go.Scatter(
    x=[0, 1],
    y=[0, 1],
    mode="lines",
    line=dict(color="gray", dash="dash"),
    showlegend=False
)

fig.add_trace(roc_curve_trace, row=1, col=2)
fig.add_trace(roc_line, row=1, col=2)

# -----------------------------------------------------------
# (c) Precision-Recall EÄŸrisi
# -----------------------------------------------------------
pr_curve_trace = go.Scatter(
    x=recall,
    y=precision,
    mode="lines",
    line=dict(color="green", width=3),
    hovertemplate="Recall: %{x:.2f}<br>Precision: %{y:.2f}<extra></extra>",
    name="Precision-Recall"
)

fig.add_trace(pr_curve_trace, row=1, col=3)

# ====================== 6. DÃœZENLEMELER ======================
# Genel baÅŸlÄ±k ve layout ayarlarÄ±
fig.update_layout(
    title_text="<b>SVM Performans Analizi (Breast Cancer)</b>",
    title_x=0.5,  # Ortaya hizalama
    font=dict(size=14),
    plot_bgcolor="white"
)

# X/Y eksen etiketleri
fig.update_xaxes(title_text="False Positive Rate", row=1, col=2)
fig.update_yaxes(title_text="True Positive Rate", row=1, col=2)

fig.update_xaxes(title_text="Recall", row=1, col=3)
fig.update_yaxes(title_text="Precision", row=1, col=3)

fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor="lightgray")
fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor="lightgray")

# GÃ¶rseli etkileÅŸimli gÃ¶stermek iÃ§in
fig.show()

```
<img src="https://github.com/ncrim7/support-vector-machines/blob/main/img/3Ekran%20Al%C4%B1nt%C4%B1s%C4%B1.PNG" width="auto" height="auto">
<img src="https://github.com/ncrim7/support-vector-machines/blob/main/img/4Ekran%20Al%C4%B1nt%C4%B1s%C4%B1.PNG" width="auto" height="auto">
<img src="https://github.com/ncrim7/support-vector-machines/blob/main/img/1Ekran%20Al%C4%B1nt%C4%B1s%C4%B1.PNG" width="auto" height="auto">
<img src="https://github.com/ncrim7/support-vector-machines/blob/main/img/2Ekran%20Al%C4%B1nt%C4%B1s%C4%B1.PNG" width="auto" height="auto">

### Kodun AÃ§Ä±klamasÄ±
1. **Veri Seti (load_breast_cancer):**  
   - `load_breast_cancer()` fonksiyonu, meme kanseri tanÄ± verilerini iÃ§eren bir veri seti dÃ¶ndÃ¼rÃ¼r.  
   - `X`, gÃ¶zlem deÄŸerlerini (hÃ¼cre Ã¶lÃ§Ã¼mleri vb.) iÃ§erirken, `y` etiket (kanser olup olmadÄ±ÄŸÄ±) bilgilerini tutar.

2. **EÄŸitim ve Test AyÄ±rma (train_test_split):**  
   - Veri seti, modelin baÅŸarÄ±sÄ±nÄ± doÄŸru Ã¶lÃ§ebilmek iÃ§in eÄŸitim ve test olarak ikiye ayrÄ±lÄ±r (%70 eÄŸitim, %30 test).  
   - `stratify=y` ile sÄ±nÄ±f dengesini (Ã¶rneklerin sÄ±nÄ±flar arasÄ±nda eÅŸit dengede olmasÄ±nÄ±) koruyoruz.

3. **Pipeline ve StandardScaler:**  
   - `StandardScaler`, Ã¶zelliklerin ortalamasÄ±nÄ± 0, varyansÄ±nÄ± 1 olacak ÅŸekilde dÃ¶nÃ¼ÅŸtÃ¼rerek veriyi Ã¶lÃ§eklendirir.  
   - ArdÄ±ndan `SVC(probability=True)` ile SVM modelini kurarÄ±z. `probability=True`, sÄ±nÄ±flandÄ±rma sonucunda olasÄ±lÄ±k deÄŸerlerini de hesaplamayÄ± saÄŸlar (Ã¶rneÄŸin ROC eÄŸrisi Ã§izmek iÃ§in gerekli olabilir).

4. **Hiperparametre Arama (Param Grid):**  
   - **`kernel`:** SVMâ€™de Ã§ekirdek fonksiyonunu belirler (`linear`, `rbf`, `poly`).  
   - **`C`:** DÃ¼zenleme (regularization) parametresi, model karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± kontrol eder.  
   - **`gamma`:** `RBF` veya `poly` kernel kullanÄ±ldÄ±ÄŸÄ±nda, veri noktalarÄ±nÄ±n ne kadar yakÄ±n olmasÄ± gerektiÄŸini belirleyen parametredir.  
   - **`degree`:** `poly` kernel iÃ§in polinom derecesidir.

5. **GridSearchCV ile Ã‡apraz DoÄŸrulama:**  
   - OluÅŸturulan her parametre kombinasyonu 5 katlÄ± Ã§apraz doÄŸrulama (cv=5) ile test edilir.  
   - `scoring="accuracy"`, en iyi modeli doÄŸruluk oranÄ±na gÃ¶re seÃ§er.  
   - `n_jobs=-1`, kullanÄ±labilir tÃ¼m iÅŸlemci Ã§ekirdeklerini kullanÄ±r (bÃ¼yÃ¼k veri setlerinde sÃ¼reyi kÄ±saltmak iÃ§in faydalÄ±).

6. **En Ä°yi Modelin EÄŸitilmesi ve DeÄŸerlendirme:**  
   - `grid_search.best_estimator_` ile en iyi bulunan model seÃ§ilir.  
   - Test veri seti Ã¼zerinde tahmin (`predict`) iÅŸlemi yapÄ±larak modelin performansÄ± Ã¶lÃ§Ã¼lÃ¼r.  
   - `classification_report`, sÄ±nÄ±f bazÄ±nda kesinlik (precision), bulma (recall) ve F1 skorunu gÃ¶sterir.  
   - `confusion_matrix`, tahmin edilen etiketler ile gerÃ§ek etiketler arasÄ±ndaki eÅŸleÅŸmeyi gÃ¶sterir.

  <img src="https://github.com/ncrim7/support-vector-machines/blob/main/img/Ekran%20Al%C4%B1nt%C4%B1s%C4%B1.PNG" width="auto" height="auto">


#### 1.3.3 KullanÄ±m AlanlarÄ±
Bu kod ile meme kanseri verilerinde SVM algoritmasÄ±nÄ±n en uygun parametrelerini arayarak daha isabetli bir teÅŸhis modeli elde edebilirsiniz. AynÄ± yÃ¶ntemi, e-posta spam tespiti, finansal veri tahminleri veya yÃ¼z tanÄ±ma gibi farklÄ± veri setlerine uyarlayarak SVMâ€™nin yaygÄ±n kullanÄ±m alanlarÄ±ndaki performansÄ±nÄ± da inceleyebilirsiniz.
### 1.4 Performans Analizi

#### 1.4.1 Uzay ve Zaman KarmaÅŸÄ±klÄ±ÄŸÄ±
Zaman KarmaÅŸÄ±klÄ±ÄŸÄ± 
SVM'nin zaman karmaÅŸÄ±klÄ±ÄŸÄ±, kullanÄ±lan Ã§ekirdek fonksiyonuna (kernel) ve veri boyutuna baÄŸlÄ± olarak 
deÄŸiÅŸir. Kodunuzda kullanÄ±lan lineer Ã§ekirdek (kernel='linear'), SVM'nin daha basit ve hÄ±zlÄ± 
Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlar. Zaman karmaÅŸÄ±klÄ±ÄŸÄ± ÅŸu ÅŸekilde hesaplanabilir: 
EÄŸitim AÅŸamasÄ± (fit): 
Zaman karmaÅŸÄ±klÄ±ÄŸÄ±, veri sayÄ±sÄ±na (n) ve Ã¶zellik sayÄ±sÄ±na (d) baÄŸlÄ±dÄ±r. 
Lineer Ã§ekirdek iÃ§in karmaÅŸÄ±klÄ±k: ğ‘¶(ğ’ğŸ â‹… ğ’…)ğ‘¶(ğ’^ğŸ \ğ’„ğ’…ğ’ğ’• ğ’…)ğ‘¶(ğ’ğŸ â‹… ğ’…) 
EÄŸer veri seti Ã§ok bÃ¼yÃ¼kse (Ã¶rneÄŸin, milyonlarca veri noktasÄ±), eÄŸitim sÃ¼resi Ã¶nemli Ã¶lÃ§Ã¼de artar. 
Tahmin AÅŸamasÄ± (predict): 
Lineer Ã§ekirdek iÃ§in her bir veri noktasÄ± tahmin edilirken zaman karmaÅŸÄ±klÄ±ÄŸÄ±:  
ï¿½
ï¿½(ğ’â‹…ğ’…) 
Genel Durum: 
Ortalama Durum: ğ‘¶(ğ’.ğ’ğ’ğ’ˆğ’) 
En KÃ¶tÃ¼ Durum: ğ‘¶(ğ’ğŸ) Ã–zellikle, veri setinin doÄŸrusal olarak ayrÄ±labilir olmadÄ±ÄŸÄ± durumlarda eÄŸitim 
sÃ¼resi artabilir. 
Uzay KarmaÅŸÄ±klÄ±ÄŸÄ± 
SVM, eÄŸitim sÄ±rasÄ±nda verileri destek vektÃ¶rleriyle temsil eder. TÃ¼m veri setini deÄŸil, yalnÄ±zca destek 
vektÃ¶rlerini saklar. 
Uzay karmaÅŸÄ±klÄ±ÄŸÄ±, veri boyutuna (n) ve Ã¶zellik sayÄ±sÄ±na (d) baÄŸlÄ±dÄ±r: O(ğ’ â‹… ğ’…) 

#### 1.4.2 Optimizasyon SeÃ§enekleri
Kernel linear yerine parabolik bÃ¶lÃ¼m yapan rbf ve polynomial olabilir. 
GridSearchCV ile fazla Ã¶ÄŸrenmeyi Ã¶nleyebiliriz. 
Test size deÄŸiÅŸebilir. Bizim Ã¶rnek iÃ§in 0.2. 
Veriler eÄŸitime girmeden fit edilebilir. 
### 1.5 Ã‡alÄ±ÅŸma SorularÄ± ve Egzersizler
##### Problem 1: 
- MÃ¼ÅŸterilerin yaÅŸÄ±, bakiyesi, son harcamalarÄ± kredi skoru gibi Ã¶zelliklerine bakarak farklÄ± etiketlerdeki 
mÃ¼ÅŸteri sÄ±nÄ±flarÄ±na ayÄ±rmak. 
##### Ã‡Ã¶zÃ¼m1: 
- Veri setindeki gerekli yerleri numerik yapÄ±p  fit edilip linear kernel ile eÄŸitme. 
##### Problem 2: 
- HastanÄ±n yaÅŸÄ±, cinsiyeti, boyu, kilosu gibi Ã¶zelliklerine bakarak hasta riski olup olmadÄ±ÄŸÄ±nÄ± hesaplama. 
##### Ã‡Ã¶zÃ¼m2: 
- Veri setindeki gerekli yerleri numerik yapÄ±p  fit edilip RBF kernel ile eÄŸitme. 
##### Problem 3: 
- El yazÄ±sÄ±ndan rakamlarÄ± tanÄ±ma. 
##### Ã‡Ã¶zÃ¼m3: 
- Veri setindeki resimlere ve verilen etiketlerine bakÄ±lÄ±r resimler arasÄ±ndaki benzerlik ile eÄŸitilir fit  edilip 
RBF kernel ile eÄŸitme. 

### 1.6 Algoritma Ã–zeti
Destek VektÃ¶r Makineleri (SVM), 1960'larda Vladimir Vapnik ve Alexey Chervonenkis tarafÄ±ndan 
geliÅŸtirilmeye baÅŸlanmÄ±ÅŸ ve 1990'larda Bernhard Boser, Isabelle Guyon ve Vladimir Vapnik tarafÄ±ndan 
tanÄ±tÄ±lan Ã§ekirdek hilesiyle (kernel trick) gerÃ§ek potansiyeline ulaÅŸmÄ±ÅŸ bir makine Ã¶ÄŸrenmesi 
algoritmasÄ±dÄ±r; doÄŸrusal ve doÄŸrusal olmayan verilerin sÄ±nÄ±flandÄ±rÄ±lmasÄ±, regresyon, anomali tespiti ve 
kÃ¼meleme gibi Ã§eÅŸitli gÃ¶revlerde kullanÄ±lÄ±r. SVM, kÃ¼Ã§Ã¼k veri setlerinde iyi performans gÃ¶stermesi, 
yÃ¼ksek boyutlu verilerle Ã§alÄ±ÅŸabilmesi, az sayÄ±da ayarlanabilir parametre gerektirmesi ve genelleme 
yeteneÄŸinin yÃ¼ksek olmasÄ± gibi avantajlara sahiptir; bu avantajlar sayesinde az sayÄ±da veri noktasÄ±yla 
bile iyi genelleme yeteneÄŸi gÃ¶sterir, Ã§ekirdek hilesi ile yÃ¼ksek boyutlu verileri dÃ¼ÅŸÃ¼k boyutlu bir uzaya 
dÃ¶nÃ¼ÅŸtÃ¼rerek karmaÅŸÄ±k problemleri Ã§Ã¶zebilir, ancak bÃ¼yÃ¼k veri setlerinde yavaÅŸ Ã§alÄ±ÅŸma, hiperparametre 
seÃ§imi, doÄŸrusal olmayan problemlerde karmaÅŸÄ±klÄ±k ve aykÄ±rÄ± deÄŸerlere duyarlÄ±lÄ±k gibi sÄ±nÄ±rlamalarÄ± da 
vardÄ±r. SVM algoritmasÄ±, veri noktalarÄ±nÄ± en bÃ¼yÃ¼k marjla ayÄ±ran bir hiper dÃ¼zlem bulmayÄ± hedefleyen 
bir optimizasyon problemi Ã§Ã¶zer; bu problem, kÄ±sÄ±tlÄ± kuadratik programlama olarak bilinir ve Ã¶zel 
algoritmalar ve yazÄ±lÄ±mlar gerektirir. Ã‡ekirdek hilesi, verileri aÃ§Ä±kÃ§a yÃ¼ksek boyutlu bir uzaya 
dÃ¶nÃ¼ÅŸtÃ¼rmeden, Ã§ekirdek fonksiyonlarÄ± aracÄ±lÄ±ÄŸÄ±yla bu dÃ¶nÃ¼ÅŸÃ¼mÃ¼ ima eder ve yÃ¼ksek boyutlu 
uzaylardaki hesaplamalar yerine Ã§ekirdek matrisi Ã¼zerinde iÅŸlemler yaparak hesaplama karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± 
azaltÄ±r; farklÄ± Ã§ekirdek fonksiyonlarÄ±, doÄŸrusal olmayan veri kÃ¼melerini baÅŸarÄ±lÄ± bir ÅŸekilde 
sÄ±nÄ±flandÄ±rmak iÃ§in kullanÄ±lÄ±r. Polinom Ã§ekirdek, iki veri noktasÄ± arasÄ±ndaki iliÅŸkiyi bir polinom 
fonksiyonu ile ifade ederken, Radyal Temel Fonksiyon (RBF) Ã§ekirdek, iki veri noktasÄ± arasÄ±ndaki Ã–klid 
uzaklÄ±ÄŸÄ±na dayalÄ± bir benzerlik Ã¶lÃ§Ã¼sÃ¼ saÄŸlar ve bu Ã§ekirdekler, doÄŸrusal olmayan verilerin daha yÃ¼ksek 
boyutlu bir uzaya dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lerek daha iyi sÄ±nÄ±flandÄ±rÄ±lmasÄ±nÄ± saÄŸlar. SVM'nin kullanÄ±m alanlarÄ± arasÄ±nda 
gÃ¶rÃ¼ntÃ¼ tanÄ±ma, el yazÄ±sÄ± tanÄ±ma, yÃ¼z tanÄ±ma, nesne tanÄ±ma, spam e-posta tespiti, duygu analizi, metin 
sÄ±nÄ±flandÄ±rma, biyomedikal uygulamalar, finansal analizler ve ses tanÄ±ma gibi Ã§eÅŸitli uygulamalar yer 
alÄ±r. Performans analizi aÃ§Ä±sÄ±ndan SVM'nin zaman ve uzay karmaÅŸÄ±klÄ±ÄŸÄ±, kullanÄ±lan Ã§ekirdek 
fonksiyonuna ve veri boyutuna baÄŸlÄ±dÄ±r; Ã¶rneÄŸin, lineer Ã§ekirdek kullanÄ±ldÄ±ÄŸÄ±nda zaman karmaÅŸÄ±klÄ±ÄŸÄ± 
ï¿½
ï¿½(ğ’ğŸ âˆ— ğ’…) ve uzay karmaÅŸÄ±klÄ±ÄŸÄ± ğ‘¶(ğ’ âˆ— ğ’…) olarak hesaplanabilir. Optimizasyon seÃ§enekleri arasÄ±nda 
farklÄ± Ã§ekirdek fonksiyonlarÄ±nÄ±n (Ã¶rneÄŸin, polinomsal veya RBF Ã§ekirdek) kullanÄ±lmasÄ±, GridSearchCV 
ile aÅŸÄ±rÄ± Ã¶ÄŸrenmenin Ã¶nlenmesi ve veri setinin eÄŸitim verisi ve test verisi olarak bÃ¶lÃ¼nmesi (Ã¶rneÄŸin, 
%20 test verisi) bulunur. AyrÄ±ca, SVM algoritmasÄ±, kod Ã¶rneÄŸi olarak verilen iris veri seti Ã¼zerinde de 
gÃ¶sterildiÄŸi gibi, eÄŸitim ve test verilerini ayÄ±rarak, lineer Ã§ekirdek ile eÄŸitim yaparak ve doÄŸruluk oranÄ±nÄ± 
hesaplayarak kullanÄ±labilir. Destek VektÃ¶r Makineleri, doÄŸru parametrelerin seÃ§ilmesi ve modelin aÅŸÄ±rÄ± 
Ã¶ÄŸrenme riskinin azaltÄ±lmasÄ± ile Ã§eÅŸitli alanlarda baÅŸarÄ±lÄ± bir ÅŸekilde uygulanabilir.
